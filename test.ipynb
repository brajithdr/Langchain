{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\Langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai,langchain\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "AIzaSyCnSjlPPjYBWPg7gLKHg_sdVYuzM1Lb-sk\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key=os.getenv(\"OPENAI_KEY\",None)\n",
    "gpalm_key=os.getenv(\"GPalm_Key\",None)\n",
    "print(api_key)\n",
    "print(gpalm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#openai_llm = OpenAI(temperature=0.9,openai_api_key=api_key)\n",
    "#response=llm.invoke(\"Suggest me a skill that is in demand?\")\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(api_key=gpalm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a model there are 1 model available\n",
    "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "model = models[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sky is not green. It is blue.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'why is the sky green?'\n",
    "text = palm.generate_text(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    temperature=0.1,\n",
    "    max_output_tokens=64,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    stop_sequences=['\\n']\n",
    ")\n",
    "text.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GooglePalm(google_api_key=gpalm_key,temperature = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hot'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\"The opposite of hot is\",'The opposite of cold is'] # according to the class prmpts must be in list\n",
    "llm_result = llm._generate(prompts)\n",
    "llm_result.generations[0][0].text\n",
    "llm_result.generations[1][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader  #load urls into docoument-loader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.indexes import VectorstoreIndexCreator #vectorize db index with chromadb\n",
    "from langchain.text_splitter import CharacterTextSplitter #text splitter\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.document_loaders import UnstructuredPDFLoader  #load pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.linkedin.com/pulse/transformers-without-pain-ibrahim-sobh-phd/',]\n",
    "#urls =['https://www.asper.ai/wp-content/uploads/2023/03/Asper_White-Paper_V003.pdf']\n",
    "loader = [UnstructuredURLLoader(urls=urls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pillow_heif'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorstoreIndexCreator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGooglePalmEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoogle_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpalm_key\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m----> 3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCharacterTextSplitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Langchain\\Lib\\site-packages\\langchain\\indexes\\vectorstore.py:82\u001b[0m, in \u001b[0;36mVectorstoreIndexCreator.from_loaders\u001b[1;34m(self, loaders)\u001b[0m\n\u001b[0;32m     80\u001b[0m docs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loader \u001b[38;5;129;01min\u001b[39;00m loaders:\n\u001b[1;32m---> 82\u001b[0m     docs\u001b[38;5;241m.\u001b[39mextend(\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_documents(docs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Langchain\\Lib\\site-packages\\langchain_community\\document_loaders\\url.py:109\u001b[0m, in \u001b[0;36mUnstructuredURLLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partition\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partition_html\n\u001b[0;32m    112\u001b[0m     docs: List[Document] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Langchain\\Lib\\site-packages\\unstructured\\partition\\auto.py:81\u001b[0m\n\u001b[0;32m     79\u001b[0m pdf_imports \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf2image\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdfminer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPIL\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(dependency_exists(dep) \u001b[38;5;28;01mfor\u001b[39;00m dep \u001b[38;5;129;01min\u001b[39;00m pdf_imports):\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partition_pdf\n\u001b[0;32m     83\u001b[0m     PARTITION_WITH_EXTRAS_MAP[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m partition_pdf\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dependency_exists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munstructured_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Langchain\\Lib\\site-packages\\unstructured\\partition\\pdf.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpdfminer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m open_filename\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m PILImage\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpillow_heif\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_heif_opener\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_chunking_strategy\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcleaners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     clean_extra_whitespace_with_index_run,\n\u001b[0;32m     43\u001b[0m     index_adjustment_after_clean_extra_whitespace,\n\u001b[0;32m     44\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pillow_heif'"
     ]
    }
   ],
   "source": [
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "        embedding=GooglePalmEmbeddings(google_api_key=gpalm_key),\n",
    "        text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m chain \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(llm\u001b[38;5;241m=\u001b[39mllm, \n\u001b[0;32m      2\u001b[0m                             chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m----> 3\u001b[0m                             retriever\u001b[38;5;241m=\u001b[39m\u001b[43mindex\u001b[49m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39mas_retriever(), \n\u001b[0;32m      4\u001b[0m                             input_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m answer \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is machine translation?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m answer\n",
      "\u001b[1;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                            chain_type=\"stuff\", \n",
    "                            retriever=index.vectorstore.as_retriever(), \n",
    "                            input_key=\"question\")\n",
    "\n",
    "\n",
    "answer = chain.run('What is machine translation?')\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\Langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'January 2023'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_folder_path = 'PDFs'\n",
    "pdf_loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]\n",
    "pdf_index = VectorstoreIndexCreator(\n",
    "        embedding=GooglePalmEmbeddings(google_api_key=gpalm_key),\n",
    "        text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(pdf_loaders)\n",
    "\n",
    "pdf_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                            chain_type=\"stuff\",\n",
    "                            retriever=pdf_index.vectorstore.as_retriever(),\n",
    "                            input_key=\"question\")\n",
    "\n",
    "pdf_answer = pdf_chain.run('When did demand planning roundtable happened?')\n",
    "pdf_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an artificial intelligence assistant work in academia. You are asked to answer questions. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "\n",
    "{question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "\n",
    "print(llm_chain.run('Who are you?'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
